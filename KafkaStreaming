Object KafkaOffsets {
    def main(args: Array[String]) {
        val brokers = "localhost:9092"
        val groupid = "GRP1"
        val topics = "KafkaOS"
        
        val SparkConf = new SparkConf().setMaster("local[*]").setAppName("KafkaStreaming")
        val ssc = new StreamingContext(SparkConf, Seconds(10))
        val sc = ssc.sparkContext
        sc.setLogLevel("OFF")
        
        val topicSet = topic.split(regex",").toSet
        val kafkaParams = Map[String, Object](
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> brokers,
            ConsumerConfig.GROUP_ID_CONFIG -> groupid,
            ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> classOf[StringDeserializer],
            ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[StrngDeserializer]
            )
            
        val messages = KafkaUtils.createDirectStream[String,String](
            ssc, LocationStrategies.PreferConsisten, ConsmerStrategies, Subscribe[String, String](topicSet,KafkaParams)
            )

/*TaskContext must be imported inorder to get the offset ranges. It give topics partitions from and to objects and so on 
*/
        messages.foreachRDD{ rdd => 
            val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
            rdd.foreachPartition{it => 
                val o: OffsetRanges = offsetRanges(TaskContext.get.partitionId)
                println(s"${o.topic} ${o.fromOffset} ${o.untilOffset}") 

            
       val line = messages.map(_.value)
       line.print()
       val words = line.flatMap(_.split(regex = " ")
       val wordCounts = words.map(x=> (x,1L)).reduceByKey(_+_)
            
       wordCounts.print()
    }
}
